# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MDRVuwgN-5twQRItSfSlkPEfoNX71xFE
"""

import nltk
import string
import pandas as pd
import nlp_utils as nu
import matplotlib.pyplot as plt
# Importing Libraries

f = open("/content/dialogs.txt",'r')
print(f.read())

df = pd.read_csv('/content/dialogs.txt', names=('Query','Response'), sep='\t')

df.head()

"""# Data Understanding"""

df.shape
# There are 3725 rows and 2 columns in our dialogs.txt dataset

df.columns
# Displaying all the columns of our dataset

df.info()
# Checking the information of the data

df.describe()
# Describe functions shows us the frequency, unique and counts of all columns

df.nunique()
# Returns number of unique elements in the object.

df.isnull().any()

df.isnull().sum()
# Checking for the presence of null values in the data.

df['Query'].value_counts()
# Checking the coounts of the values present in the columns 'Query'

df['Response'].value_counts()
# Checking the counts of the values present in the column 'Response'

"""# Data Visualization"""

import nltk
nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

Text = df['Query']
Text[0]

for sentence in Text:
  print(sentence)

  ss = sia.polarity_scores(sentence)
  for k in ss:
    print('{0}: {1}, '.format(k,ss[k]),end="")
    print()

k,ss[k]

analyzer = SentimentIntensityAnalyzer()
df['rating'] = Text.apply(analyzer.polarity_scores)
df.head()

df = pd.concat([df.drop(['rating'],axis=1), df['rating'].apply(pd.Series)], axis=1)

df

from wordcloud import WordCloud
# importing word cloud

def wordcloud(df, label):
  subset = df[df[label]==1]
  text = df.Query.values
  wc = WordCloud(background_color="black",max_words=1000)

  wc.generate(" ".join(text))

  plt.figure(figsize=(20,20))
  plt.subplot(221)
  plt.axis("off")
  plt.title("words frequented in {}".format(label),fontsize=20)
  plt.imshow(wc.recolor(colormap='gist_earth', random_state=244),alpha=0.98)
  # visualising wordcloud

wordcloud(df,'Query')
# top words in the query column

wordcloud(df,'Response')

"""# Text-Normalization"""

import re
punc_lower = lambda x:re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())
# lower case conversion

remove_n = lambda x: re.sub("\n", " ", x)
# removing \n and replacing them with empty value

remove_non_ascii = lambda x: re.sub(r'[^\x00-\x7f]',r' ', x)
# removing non ascii characters

alphanumeric = lambda x: re.sub('\w*\d\w*', ' ', x)
# removing alpha numeric values

df['Query'] = df['Query'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)
# using map function and applying the function on query column

df['Response'] = df['Response'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)
# using map function and applying the function on response column

df

pd.set_option('display.max_rows',3800)
# Displaying all rows in the dataset

df

"""# Important Sentence"""

imp_sent=df.sort_values(by='compound', ascending=False)
# arranging the compound column in descending order to find the best sentence.

imp_sent.head()

"""# Top Positive Sentence"""

pos_sent=df.sort_values(by='pos', ascending=False)
# Arranging the dataframe by positive column in descending order to find the best postive sentence.

pos_sent.head()

"""# Top negative sentence"""

neg_sent=df.sort_values(by='neg', ascending=False)
# Arranging the dataframe by negative column in descending order to find the best negative sentence.

neg_sent.head()

"""# Top Neutral Sentence"""

neu_sent=df.sort_values(by='neu', ascending=False)
# Arranging the dataframe by negative column in descending order to find the best neutral sentence.

neu_sent.head()

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()

factors = tfidf.fit_transform(df['Query']).toarray()

tfidf.get_feature_names()

"""# Application"""

from sklearn.metrics.pairwise import cosine_distances

query = 'who are you ?'
def chatbot(query):
    # step:-1 clean
    query = lemma.lemmatize(query)
    # step:-2 word embedding - transform
    query_vector = tfidf.transform([query]).toarray()
    # step-3: cosine similarity
    similar_score = 1 -cosine_distances(factors,query_vector)
    index = similar_score.argmax() # take max index position
    # searching or matching question
    matching_question = df.loc[index]['Query']
    response = df.loc[index]['Response']
    pos_score = df.loc[index]['pos']
    neg_score = df.loc[index]['neg']
    neu_score = df.loc[index]['neu']
    confidence = similar_score[index][0]
    chat_dict = {'match':matching_question,
                'response':response,
                'score':confidence,
                'pos':pos_score,
                'neg':neg_score,
                'neu':neu_score}
    return chat_dict

chatbot(query)

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemma = WordNetLemmatizer()

while True:
    query = input('USER: ')
    if query == 'exit':
        break
        
    response = chatbot(query)
    if response['score'] <= 0.2: # 
        print('BOT: Please rephrase your Question.')
    
    else:
        print('='*80)
        print('logs:\n Matched Question: %r\n Confidence Score: %0.2f \n PositiveScore: %r \n NegativeScore: %r\n NeutralScore: %r'%(
            response['match'],response['score']*100,response['pos'],response['neg'],response['neu']))
        print('='*80)
        print('BOT: ',response['response'])